{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 16\n",
    "# Spark with Python Examples \n",
    "Today we will see examples on how to use PySpark (the Spark API for Python) to handle data. In particular, we will learn how to create RDD objects, perform lazy evaluation, and use data frames to query from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Dataset (RDD)\n",
    "\n",
    "Creating a resilient distributed dataset (RDD) is a common entry point for Spark users. In this section, we will create RDDs using `SparkContext` class and learn its important functions: `map()`, `reduce()`, `filter()`, `collect()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "sc = SparkContext(master=\"local[4]\") # 4 means assigning 4 computational cores for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RDD with 20 random integers.\n",
    "lst = np.random.randint(0, 10, 20)\n",
    "print(lst)\n",
    "A = sc.parallelize(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A is an RDD object\n",
    "type(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data back to a regular list.\n",
    "A.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A splits the data into 4 pieces.\n",
    "A.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what happens if we assign 2 cores instead.\n",
    "\n",
    "sc.stop()  # stop current spark environment\n",
    "\n",
    "sc = SparkContext(master=\"local[2]\")  # Assign 2 cores\n",
    "\n",
    "A = sc.parallelize(lst)\n",
    "A.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `map()` method applies a function to the values contained in one RDD and creates a new RDD with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create a new RDD by squaring the values contained in A. \n",
    "B = A.map(lambda x: x * x)  # the mapping is defined by a lambda expression\n",
    "B.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reduce()` method creates a new RDD by aggregating values in an RDD by certain rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of the data.\n",
    "A.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum value.\n",
    "A.reduce(lambda x,y: x if x > y else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `filter()` creates a new RDD with data satisfying certain conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all integers in A that are divisible by 3.\n",
    "A.filter(lambda x: x % 3 ==0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Evaluation\n",
    "\n",
    "Lazy evaluation is an computation strategy which prepares a detailed step-by-step internal map of the computations, but delays the final execution until when it is absolutely needed. This strategy can allow PySpark to optimize the way computations are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rdd1 = sc.parallelize(range(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that takes time to compute.\n",
    "def hardtask(x):\n",
    "    [np.cos(j) for j in np.arange(100)]\n",
    "    return np.cos(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing `hardtask()` takes a wall time of 6.97 ms on my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "hardtask(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply `hardtask()` to all values in `rdd1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = rdd1.map(lambda x: hardtask(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of lazy evaluation, nothing was computed yet. Spark only makes a plan of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy evaluations postpones the execution until it is necessary. It gives room the Spark to optimize the order of computations and improved the execution efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Data Frames\n",
    "A **Spark DataFrame** is a distributed collection of rows under named columns. It is conceptually equivalent to data frames provided by Pandas, but it is constructed upon data formats such as RDDs so that it can handle large amount of data efficiently.\n",
    "\n",
    "We need to keep in mind that Spark DataFrame is immutable, which means that we can't change a data frame once it is created. In most cases, we need to create a new data frame after applying transformations to an existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Download MovieLens 25M dataset\n",
    "url = \"http://files.grouplens.org/datasets/movielens/ml-25m.zip\"\n",
    "urllib.request.urlretrieve(url, 'Data/ml-25m.zip')\n",
    "with zipfile.ZipFile('Data/ml-25m.zip', \"r\") as file:\n",
    "    file.printdir()\n",
    "    file.extractall('Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, desc , col, max, struct, mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SparkSession()` is an environment introduced in Spark 2.0. It is the main entry point for creating data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new spark session\n",
    "spark = SparkSession.builder.appName('spark_app').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ratings.csv file:\n",
    "path = \"Data/ml-25m/ratings.csv\"\n",
    "ratings = spark.read.format('csv')\\\n",
    "            .option('inferSchema', True)\\  # Let spark decide the data types of each column\n",
    "            .option('header', True)\\       # Use the first row as column names\n",
    "            .load(path)                    # Specify the source of data\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Delete\" the timestamp column\n",
    "ratings = ratings.drop('timestamp')\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data types\n",
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the data frame\n",
    "print(ratings.count(), len(ratings.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Select all ratings of movie 1.\n",
    "q1 = ratings.select('*').filter(ratings.movieId == 1)\n",
    "q1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: top 10 most-rated movies\n",
    "ratings_count = ratings.groupby('movieId').agg(count('*').alias('count'))\n",
    "ratings_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = ratings_count.orderBy(desc('count')).limit(10)\n",
    "q2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise:\n",
    "# Query 3: find top 10 users with most ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's merge the rating data with the movies data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Data/ml-25m/movies.csv'\n",
    "\n",
    "# Load movies.csv as a Spark data frame named movies\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "movies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge movies with ratings\n",
    "data = ratings.join(movies, how='inner', on=['movieId'])\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise:\n",
    "# Merge q2 with movies to find out the title of the 10 most-rated movies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 4: Find the average rating of each movie\n",
    "avg_ratings = data.select('rating', 'title')\\\n",
    "        .groupby('title')\\\n",
    "        .agg(mean('rating').alias('AvgRating'))\n",
    "avg_ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ratings = avg_ratings.orderBy(desc('AvgRating'))\n",
    "avg_ratings.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 5: Select movies with at least 50 ratings.\n",
    "movie_rating_count = data.groupby('title')\\\n",
    "                        .agg(count('*').alias('NumRatings'))\\\n",
    "                        .orderBy(desc('NumRatings'))\n",
    "movie_rating_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_rating_count = movie_rating_count.filter(movie_rating_count.NumRatings >= 50)\n",
    "movie_rating_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the above data frame with avg_ratings\n",
    "movie_ratings = movie_rating_count.join(avg_ratings, how='left', on=['title'])\n",
    "movie_ratings.show(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise:\n",
    "# Find the top 10 highly-rated movies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
